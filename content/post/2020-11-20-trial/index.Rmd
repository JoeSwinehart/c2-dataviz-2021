---
title: Trial
author: Daniel Anderson
date: '2020-11-20'
slug: trial
categories:
  - dataviz
  - geo-spatial
tags: []
toc: true
---

Here's a post about some stuff

```{r }
library(tidyverse)
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_smooth() +
  theme_minimal()
```

## Leve 2 header
And here's a table

```{r }
mpg %>% 
  knitr::kable()
```

Feature engineering is a fancy machine learning way of saying "prepare your data for analysis". But it's also more than just getting your data in the right format. It's about transforming variables to help the model provide more stable predictions -- for example, encoding, modeling, or omitting missing data; creating new variables from the available dataset; and otherwise helping improve the model performance through transformations and modifications to the existing data *before* analysis.

One of the biggest challenges with feature engineering is what's referred to as **data leakage**, which occurs when information from the test dataset leaks into the training dataset. Consider the simple example of normalizing or standardizing a variable (subtracting the variable mean from each value and then dividing by the standard deviation). Normalizing your variables is necessary for a wide range of predictive models, including any model using regularization methods, principal components analysis, and $k$-nearest neighbor models, among others. But when we normalize the variable, it is critical we do so relative to the training set, *not* relative to the full dataset. If we normalize the numeric variables in our full dataset and then divide it into test and training datasets, information from the test dataset has *leaked* into the training dataset. This seems simple enough - just wait to standardize until after splitting - but it becomes more complicated when we consider *tuning* a model. If we use a process like $k$-fold cross-validation, then we have to normalize *within* each fold to get an accurate representation of out-of-sample predictive accuracy.

<!-- In the above, things like k-fold cv will probably have chapters of their own. We should link to them in the text -->

In this chapter, we'll introduce feature engineering using the [**{recipes}**](https://recipes.tidymodels.org) package from the [**{tidymodels}**](https://www.tidymodels.org) suite of packages which, as we will see, helps you to be explicit about the decisions you're making, while avoiding potential issues with data leakage.

## Basics of {recipes}

![](https://github.com/tidymodels/recipes/raw/master/man/figures/logo.png)

The **{recipes}** package is designed to replace the [stats::model.matrix()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/model.matrix.html) function that you might be familiar with. For example, if you fit a model like the one below

```{r}
library(palmerpenguins)
m1 <- lm(bill_length_mm ~ species, data = penguins)
summary(m1)
```

You can see that our `species` column, which has the values `r unique(penguins$species)`, is automatically dummy-coded for us, with the first level in the factor variable (Adelie) set as the reference group. The **{recipes}** package forces you to be a bit more explicit in these decisions. But it also has a much wider range of modifications it can make to the data. 

Using **{recipes}** also allows you to more easily separate the pre-processing and modeling stages of your data analysis workflow. In the above example, you may not have even realized `stats::model.matrix()` was doing anything for you because it's wrapped within the `stats::lm()` modeling code. But with **{recipes}**, you make the modifications to your data first and *then* conduct your analysis.

How does this separation work? With **{recipes}**, you can create a blueprint (or recipe) to apply to a given dataset, without actually applying those operations. You can then use this blueprint iteratively across sets of data (e.g., folds) as well as on new (potentially unseen) data that has the same structure (variables). This process helps avoid data leakage because all operations are carried forward and applied together, and no operations are conducted until explicitly requested.

### Creating a recipe

Let's read in some data and begin creating a basic recipe. We'll work with the simulated statewide testing data introduced previously. This is a fairly decent sized dataset, and since we're just illustrating concepts here, we'll pull a random sample of 2% of the total data to make everything run a bit quicker. We'll also remove the `classification` variable, which is just a categorical version of `score`, our outcome.

In the chunk below, we read in the data, sample a random 2% of the data (being careful to set a seed first so our results are reproducible), split it into training and test sets, and extract just the training dataset. We'll hold off on splitting it into CV folds for now.